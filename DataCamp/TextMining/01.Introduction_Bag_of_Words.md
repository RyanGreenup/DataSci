---
pinned: false
tags: [Notebooks/Programming/R/Text Mining, Notebooks/Social Web Analytics/Data Camp]
title: Text Mining Using Bag of Words
created: '2020-02-12T10:42:06.320Z'
modified: '2020-01-29T23:24:48.470Z'
toc: true
---

[[toc]]

# Text Mining Using Bag of Words 

Refer to the [Slides here](../../../MD/attachments/DataSci/TextMining/TextminingBagWordsCh1.pdf)

## Preamble

The following code will set up a nice working environment:





## (01) Jumping into Text Mining with bag of words

### What is text Mining

#### General outline

The idea of text mining is to reduce the amount of information, often there is
too much information to work with. The goal is to reduce information and
highlight what's important.

A Text mining work-flow looks like this:

![](./)
/home/ryan/Dropbox/Notes/MD/Programming/R/Text_Mining/01.Introduction_Bag_of_Words.md
There is a work-flow to follow:

1. Define the problem and specific goals
2. Identify the text to be collected
3. Organize the text
4. Extract Features from the text
5. Analyse the Text
6. Make a Conclusion



#### Types of Text Mining Strategies

##### Semantic parsing

This is based on word syntax, you are interested in:

1. Word type
2. Word Order

![Image](./_v_attachments/SemanticParsing.png)

Semantic parsing follows a tree structure to break up the text and apply tags to
the text, such as adjective, part of a sentence, name etc.

So semantic parsing is feature rich because there are a lot of output variables.

##### Bag of Words
The bag of words method does not care about word type or order and instead words
are just attributes of the document.

[This Course](https://campus.datacamp.com/courses/intro-to-text-mining-bag-of-words/jumping-into-text-mining-with-bag-of-words?ex=1)
is only concerned with the bag of words method, later courses in the
[Text mining skill Track](https://www.datacamp.com/tracks/text-mining-with-r)
deal with semantic parsing.

### Quick Taste of Text Mining
Some times it is sufficient to look at word frequency only, for example:


```r
text <- "Text mining usually involves the process of structuring the input text. The overarching goal is, essentially, to turn text into data for analysis, via application of natural language processing (NLP) and analytical methods."

new_text <- "DataCamp is the first online learning platform that focuses on building the best learning experience specifically for Data Science. We have offices in New York, London and Belgium and to date, we trained over 3.8 million (aspiring) data scientists in over 150 countries. These data science enthusiasts completed more than 185 million exercises. You can take free beginner courses, or subscribe for $29/month to get access to all premium courses."

# Load qdap
library(qdap)

# Print new_text to the console
new_text
```

```
## [1] "DataCamp is the first online learning platform that focuses on building the best learning experience specifically for Data Science. We have offices in New York, London and Belgium and to date, we trained over 3.8 million (aspiring) data scientists in over 150 countries. These data science enthusiasts completed more than 185 million exercises. You can take free beginner courses, or subscribe for $29/month to get access to all premium courses."
```

```r
# Find the 10 most frequent terms: term_count
term_count <- freq_terms(new_text, 4)
summary(term_count)
```

```
##      WORD                FREQ      
##  Length:12          Min.   :2.000  
##  Class :character   1st Qu.:2.000  
##  Mode  :character   Median :2.000  
##                     Mean   :2.167  
##                     3rd Qu.:2.000  
##                     Max.   :3.000
```

```r
head(term_count)
```

```
##   WORD    FREQ
## 1 data       3
## 2 to         3
## 3 and        2
## 4 courses    2
## 5 for        2
## 6 in         2
```

```r
# Plot term_count
plot(term_count)
```

![plot of chunk unnamed-chunk-1](figure/unnamed-chunk-1-1.png)

```r
# Plot Using GGPlot2
## Col (use this)
 ggplot(data = term_count, aes(x = WORD, y = FREQ, col = -FREQ, fill = WORD)) +
    geom_col() +
    guides(col = FALSE, fill = FALSE)
```

![plot of chunk unnamed-chunk-1](figure/unnamed-chunk-1-2.png)

it's easy to tell that this is bag of words not semantic because the type of words isn't considered by the function that was used.

### Getting Started

A corpus is a collection of documents, so first it is necessary to load in data and determine which columns correspond to features.
When importing data it is important to specify `stringsAsFactors = FALSE` because otherwise the `read.csv()` function will import `character` strings as `factor` levels.


```r
tweets <- read.csv("~/Dropbox/Notes/DataSci/0DataSets/TextMining/coffee.csv",
    stringsAsFactors = FALSE)
str(tweets)
```

```
## 'data.frame':	1000 obs. of  15 variables:
##  $ num         : int  1 2 3 4 5 6 7 8 9 10 ...
##  $ text        : chr  "@ayyytylerb that is so true drink lots of coffee" "RT @bryzy_brib: Senior March tmw morning at 7:25 A.M. in the SENIOR lot. Get up early, make yo coffee/breakfast"| __truncated__ "If you believe in #gunsense tomorrow would be a very good day to have your coffee any place BUT @Starbucks Guns"| __truncated__ "My cute coffee mug. http://t.co/2udvMU6XIG" ...
##  $ favorited   : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
##  $ replyToSN   : chr  "ayyytylerb" NA NA NA ...
##  $ created     : chr  "8/9/2013 2:43" "8/9/2013 2:43" "8/9/2013 2:43" "8/9/2013 2:43" ...
##  $ truncated   : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
##  $ replyToSID  : num  3.66e+17 NA NA NA NA ...
##  $ id          : num  3.66e+17 3.66e+17 3.66e+17 3.66e+17 3.66e+17 ...
##  $ replyToUID  : int  1637123977 NA NA NA NA NA NA 1316942208 NA NA ...
##  $ statusSource: chr  "<a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>" "<a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>" "web" "<a href=\"http://twitter.com/download/android\" rel=\"nofollow\">Twitter for Android</a>" ...
##  $ screenName  : chr  "thejennagibson" "carolynicosia" "janeCkay" "AlexandriaOOTD" ...
##  $ retweetCount: int  0 1 0 0 2 0 0 0 1 2 ...
##  $ retweeted   : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
##  $ longitude   : logi  NA NA NA NA NA NA ...
##  $ latitude    : logi  NA NA NA NA NA NA ...
```

```r
# isolate the text
coffee_tweets <- tweets$text
```

Now that the data is loaded in as a vector, it is necessary to convert the vector containing the text into a `corpus` data type. a `corpus` is a collection of documents but **_R_** recognizes it as a data type. 

A corpus can be stored as a *volatile corpus* in memory as `VCorpus` as well as *Permanent corpus* which is saved to disk as `PCorpus` 

to make a *volatile Corpus* **_R_** needs to interpret each element from the vector of text `coffee_tweets` as a as a document, the `tm` package provides `source`  functions to do that:


```r
library(tm)
coffee_source <- tm::VectorSource(coffee_tweets)
coffee_source  %>% head()
```

```
## [1] "@ayyytylerb that is so true drink lots of coffee"                                                                                            
## [2] "RT @bryzy_brib: Senior March tmw morning at 7:25 A.M. in the SENIOR lot. Get up early, make yo coffee/breakfast, cus this will only happen ?"
## [3] "If you believe in #gunsense tomorrow would be a very good day to have your coffee any place BUT @Starbucks Guns+Coffee=#nosense @MomsDemand" 
## [4] "My cute coffee mug. http://t.co/2udvMU6XIG"                                                                                                  
## [5] "RT @slaredo21: I wish we had Starbucks here... Cause coffee dates in the morning sound perff!"                                               
## [6] "Does anyone ever get a cup of coffee before a cocktail??"
```

Now that the vector is a source object it needs to be passed to `tm::VCorpus()` in order to create a *volatile corpus* object:

> The `VCorpus` object is a nested list, at each index there is a `PlainTextDocument` object which is a list containing text data known as `content` and some medadata known as `meta`, so in order to access the 15th document you would use `coffee_corpus[[15]]` and then the `content` or `meta` could be accessed with `[1]` or `[2]`.  


```r
# Make the Corpus Object
coffee_corpus <- tm::VCorpus(coffee_source)

# Print the object
print(coffee_corpus)
```

```
## <<VCorpus>>
## Metadata:  corpus specific: 0, document level (indexed): 0
## Content:  documents: 1000
```

```r
# Print the 15th tweet
print(coffee_corpus[[15]])[1]
```

```
## <<PlainTextDocument>>
## Metadata:  7
## Content:  chars: 111
```

```
## $content
## [1] "@HeatherWhaley I was about 2 joke it takes 2 hands to hold hot coffee...then I read headline! #Don'tDrinkNShoot"
```

```r
# or use content()
NLP::content(coffee_corpus[[15]])
```

```
## [1] "@HeatherWhaley I was about 2 joke it takes 2 hands to hold hot coffee...then I read headline! #Don'tDrinkNShoot"
```

### Make a VCorpus from a data Frame

If the text data is in a data frame you just use the `DataframeSource()` rather than extracting the vector. The data must have a specific structure:

* Column **One** must be called `doc_id` and each row must have a unique string.
* Column **two** must be called `text` with `UTF-*` encoding
* All following columns will be metadata.


```r
# Create a dummy data frame
example_text <- data.frame(doc_id = 1:3, text = c("Text mining is fun", "Text analysis provides insights", "qdap and tm are used in text mining"), author = c("John", "Jim", "Bill"), date = c(1514953399, 1514866998, 1514680598)) 
example_text
```

```
##   doc_id                                text author       date
## 1      1                  Text mining is fun   John 1514953399
## 2      2     Text analysis provides insights    Jim 1514866998
## 3      3 qdap and tm are used in text mining   Bill 1514680598
```

```r
# Create a text Source File
df_source <- tm::DataframeSource(example_text)
class(df_source) 
```

```
## [1] "DataframeSource" "SimpleSource"    "Source"
```

```r
head(df_source) 
```

```
##   doc_id                                text author       date
## 1      1                  Text mining is fun   John 1514953399
## 2      2     Text analysis provides insights    Jim 1514866998
## 3      3 qdap and tm are used in text mining   Bill 1514680598
```

```r
# Create a Volatile Corpus
df_corpus <- tm::VCorpus(df_source)
print(df_corpus)
```

```
## <<VCorpus>>
## Metadata:  corpus specific: 0, document level (indexed): 2
## Content:  documents: 3
```

```r
# Examine the Meda Data 
NLP::meta(df_corpus)
```

```
##   author       date
## 1   John 1514953399
## 2    Jim 1514866998
## 3   Bill 1514680598
```

### Cleaning and Preprocessing Text

Now that we have a corpus we need to start cleaning up the raw data

![Image](../attachments/CommonPreProcessingFunctions.png)

The *bag of words* text mining approach is such that cleaning helps to aggregate terms, for example it makes sense for the words `mine` , `miner`  and `mining` to all simply be considered as the word `mine`.

Common preprocessing functions inlcude:

* `base::tolower()` 
* `tm::removePunctuation()` 
* `tm::removeNumbers()` 
* `tm::stripWhitespace()` 


```r
text <- "<b>She</b> woke up at       6 A.M. It\'s so early!  She was only 10% awake and began drinking coffee in front of her computer."

# Make lowercase
tolower(text)
```

```
## [1] "<b>she</b> woke up at       6 a.m. it's so early!  she was only 10% awake and began drinking coffee in front of her computer."
```

```r
# Remove punctuation
tm::removePunctuation(text)
```

```
## [1] "bSheb woke up at       6 AM Its so early  She was only 10 awake and began drinking coffee in front of her computer"
```

```r
# Remove numbers
tm::removeNumbers
```

```
## function (x, ...) 
## UseMethod("removeNumbers")
## <bytecode: 0x55e0365ef330>
## <environment: namespace:tm>
```

```r
# Remove whitespace
tm::stripWhitespace(text)
```

```
## [1] "<b>She</b> woke up at 6 A.M. It's so early! She was only 10% awake and began drinking coffee in front of her computer."
```

```r
# Convert it from HTML to LaTeX
if (grep(pattern = '<b>', x = text)) {
    textTeX <- text
    textTeX <- gsub(pattern = '<br>' , replacement = "\\\\"  , textTeX)
    textTeX <- gsub(pattern = '<b>'  , replacement = "\\\textbf{", textTeX)
    textTeX <-  gsub(pattern = '</b>', replacement = "}"     , textTeX)
    # Huh % is hard
    textTeX <-  gsub(pattern = '%', replacement = "\\\\%"     , textTeX)
    print(textTeX)
}
```

```
## [1] "\textbf{She} woke up at       6 A.M. It's so early!  She was only 10\\% awake and began drinking coffee in front of her computer."
```

### Cleaning with `qdap`

the `qdap` package offers other text cleaning functions:

* `bracketX()` 
  * will remove text within brackets
* replace_number()
  * turn numbers into words
* replace_abbreviation()
  * Create Full text from abbreviations.
* replace_contraction()
* replace_symbols()
  * turn words into symbols



```r
text <- "Text mining usually involves the process of structuring the input text. The overarching goal is, essentially, to turn text into data for analysis, via application of natural language processing (NLP) and analytical methods."

## text is still loaded in your workspace

# Remove text within brackets
qdap::bracketX(text.var = text)
```

```
## [1] "Text mining usually involves the process of structuring the input text. The overarching goal is, essentially, to turn text into data for analysis, via application of natural language processing and analytical methods."
```

```r
# Replace numbers with words
qdap::replace_number(text)
```

```
## [1] "Text mining usually involves the process of structuring the input text. The overarching goal is, essentially, to turn text into data for analysis, via application of natural language processing (NLP) and analytical methods."
```

```r
# Replace abbreviations
qdap::replace_abbreviation(text)
```

```
## [1] "Text mining usually involves the process of structuring the input text. The overarching goal is, essentially, to turn text into data for analysis, via application of natural language processing (NLP) and analytical methods."
```

```r
# Replace contractions
qdap::replace_contraction(text)
```

```
## [1] "Text mining usually involves the process of structuring the input text. The overarching goal is, essentially, to turn text into data for analysis, via application of natural language processing (NLP) and analytical methods."
```

```r
# Replace symbols with words
qdap::replace_symbol(text)
```

```
## [1] "Text mining usually involves the process of structuring the input text. The overarching goal is, essentially, to turn text into data for analysis, via application of natural language processing (NLP) and analytical methods."
```


### All about Stop Words
Some words are frequent but more or less meaningless with respect to the *bag of words* method, these words include things like `I`, `the`, `will` et cetera.

Another example is a word for which meaning has already been accounted, in this case all the tweets are to do with `coffee` , so it will occur at a high rate of frequency and will demean any other possible insights that might be drawn from this analysis.

new words can be removed by using the vector `all_stops`: 


```r
text <- "Text mining usually involves the process of structuring the input text. The overarching goal is, essentially, to turn text into data for analysis, via application of natural language processing (NLP) and analytical methods."

# List standard English stop words
tm::stopwords("en")
```

```
##   [1] "i"          "me"         "my"         "myself"     "we"        
##   [6] "our"        "ours"       "ourselves"  "you"        "your"      
##  [11] "yours"      "yourself"   "yourselves" "he"         "him"       
##  [16] "his"        "himself"    "she"        "her"        "hers"      
##  [21] "herself"    "it"         "its"        "itself"     "they"      
##  [26] "them"       "their"      "theirs"     "themselves" "what"      
##  [31] "which"      "who"        "whom"       "this"       "that"      
##  [36] "these"      "those"      "am"         "is"         "are"       
##  [41] "was"        "were"       "be"         "been"       "being"     
##  [46] "have"       "has"        "had"        "having"     "do"        
##  [51] "does"       "did"        "doing"      "would"      "should"    
##  [56] "could"      "ought"      "i'm"        "you're"     "he's"      
##  [61] "she's"      "it's"       "we're"      "they're"    "i've"      
##  [66] "you've"     "we've"      "they've"    "i'd"        "you'd"     
##  [71] "he'd"       "she'd"      "we'd"       "they'd"     "i'll"      
##  [76] "you'll"     "he'll"      "she'll"     "we'll"      "they'll"   
##  [81] "isn't"      "aren't"     "wasn't"     "weren't"    "hasn't"    
##  [86] "haven't"    "hadn't"     "doesn't"    "don't"      "didn't"    
##  [91] "won't"      "wouldn't"   "shan't"     "shouldn't"  "can't"     
##  [96] "cannot"     "couldn't"   "mustn't"    "let's"      "that's"    
## [101] "who's"      "what's"     "here's"     "there's"    "when's"    
## [106] "where's"    "why's"      "how's"      "a"          "an"        
## [111] "the"        "and"        "but"        "if"         "or"        
## [116] "because"    "as"         "until"      "while"      "of"        
## [121] "at"         "by"         "for"        "with"       "about"     
## [126] "against"    "between"    "into"       "through"    "during"    
## [131] "before"     "after"      "above"      "below"      "to"        
## [136] "from"       "up"         "down"       "in"         "out"       
## [141] "on"         "off"        "over"       "under"      "again"     
## [146] "further"    "then"       "once"       "here"       "there"     
## [151] "when"       "where"      "why"        "how"        "all"       
## [156] "any"        "both"       "each"       "few"        "more"      
## [161] "most"       "other"      "some"       "such"       "no"        
## [166] "nor"        "not"        "only"       "own"        "same"      
## [171] "so"         "than"       "too"        "very"
```

```r
# Print text without standard stop words
tm::removeWords(text, stopwords("en"))
```

```
## [1] "Text mining usually involves  process  structuring  input text. The overarching goal , essentially,  turn text  data  analysis, via application  natural language processing (NLP)  analytical methods."
```

```r
# Add "coffee" and "bean" to the list: new_stops
new_stops <- c("coffee", "bean", stopwords("en"))

# Remove stop words from text
tm::removeWords(text, new_stops)
```

```
## [1] "Text mining usually involves  process  structuring  input text. The overarching goal , essentially,  turn text  data  analysis, via application  natural language processing (NLP)  analytical methods."
```

### Intro to word stemming and stem completion

the `tm`  package provides the `stemDocument()` function to get to a word's root.
This function either:

* `character` vector -->  `character` vector`character` vector
* `PlainTextDocument` --> `PlainTextDocument`  

Then `stemCompletion()` can be used to reconstruct the words back into a known term.
`stemCompletion()` accepts a character vector and a completion dictionary.


```r
library(SnowballC)
# Create complicate
complicate <- c("complicated", "complication", "complicatedly") 

# Perform word stemming: stem_doc
stem_doc <- tm::stemDocument(complicate)

# Create the completion dictionary: comp_dict
comp_dict <- "complicate"

# Perform stem completion: complete_text 
complete_text <- tm::stemCompletion(stem_doc, comp_dict)


# Print complete_text
complete_text
```

```
##      complic      complic      complic 
## "complicate" "complicate" "complicate"
```

### Word stemming and stem completion on a sentence
Sentences are considered by **_R_** as a single character vector, before word stemming can be used it is necessary to split the sentence up into multiple words:


```r
text_data <- "In a complicated haste, Tom rushed to fix a new complication, too complicatedly."

# Remove punctuation: rm_punc
rm_punc <- tm::removePunctuation(text_data)

# Create character vector: n_char_vec
n_char_vec <- unlist(strsplit(rm_punc, split = " "))

# Perform word stemming: stem_doc
stem_doc <- tm::stemDocument(n_char_vec)

# Create the completion Dictionary
comp_dict  <- c("In", "a", "complicate", "haste", "Tom", "rush", "to", "fix", "new", "too")

# Print stem_doc
stem_doc
```

```
##  [1] "In"      "a"       "complic" "hast"    "Tom"     "rush"    "to"     
##  [8] "fix"     "a"       "new"     "complic" "too"     "complic"
```

```r
# Re-complete stemmed document: complete_doc
complete_doc <- tm::stemCompletion(stem_doc, comp_dict)

# Print complete_doc
complete_doc
```

```
##           In            a      complic         hast          Tom 
##         "In"          "a" "complicate"      "haste"        "Tom" 
##         rush           to          fix            a          new 
##       "rush"         "to"        "fix"          "a"        "new" 
##      complic          too      complic 
## "complicate"        "too" "complicate"
```


### Apply preprocessing steps to a corpus

the `tm`  package provides the `tm_map()`  function to apply cleaning functions to an entire corpus, making the cleaning steps easier. It takes two argumens:

1. A `corpus` object
2. A cleaning function (like `removeNumbers()` 
   * If cleaning functions come from base **_R_** or `qdap` rather than `tm` it is necessary to wrap them in `content_transformer()`.

It is often more appropriate to write this is a function and then call the function.


```r
clean_corpus <- function (corpus) {
    # Remove Punctuation
    corpus <- tm_map(corpus, removePunctuation)

    # Transform to Lower Case
    corpus <- tm_map(corpus, content_transformer(tolower))

    # Add more Stopwords
    corpus <- tm_map(corpus, removeWords, words = c(stopwords("en"), "coffee", "mug"))

    # Strip Whitespace
    corpus <- tm_map(corpus, stripWhitespace)

    return(corpus)
}

clean_corp <- clean_corpus(df_corpus)
print(clean_corp[[2]])
```

```
## <<PlainTextDocument>>
## Metadata:  7
## Content:  chars: 31
```

### The TDM and DTM

With the text in a clean form it is necessary to give it structure, as far as *bag of text* text mining is concenerned, either the: 

* Term Document Matrix (TDM), OR
* Document Term Matrix (DTM)
  * These are useful when you want to preserve time series for chronological data.

is used.

Think **Row**-**Col**-Matrix, where a term is extracted word and the column is the document (i.e. the tweet).

A TDM looks like this:

| **Tweet 1** | **Tweet 2** | **Tweet 3** | **Tweet 4** |
| ---         |         --- |         --- |         --- |
| **Term 1**  |           0 |           0 |           0 |
| **Term 2**  |           1 |           1 |           0 |
| **Term 3**  |           1 |           1 |           0 |
| **Term 4**  |           0 |           0 |           3 |
| **Term 5**  |           0 |           0 |           1 |

A DTM looks like this:

| **Term 1**  | **Term 1** | **Term 2** | **Term 3** | **Term 4** | **Term 5** |
| ---         |        --- |        --- |        --- |        --- |        --- |
| **Tweet 1** |          2 |          1 |          1 |          0 |          0 |
| **Tweet 2** |          0 |          1 |          1 |          0 |          0 |
| **Tweet 3** |          0 |          1 |          0 |          0 |          0 |
| **Tweet 4** |          0 |          0 |          0 |          3 |          1 |

They can be created by using either the `TermDocumentMatrix`  or `DocumentTermMatrix` functions.

The `qdap` package relies on the word frequency matrix, but this is less popular and so wont be used. it's basically just a matrix with only one column.

#### Application to Coffee Dataset
So now we'll apply this to the coffee dataset by first importing the data and arranging the columns


```r
# Load the Data Set
coffee <- read.csv(file = "../../0DataSets/TextMining/coffee.csv",
		   stringsAsFactors = FALSE) 

# Investigate the Data
str(coffee)
```

```
## 'data.frame':	1000 obs. of  15 variables:
##  $ num         : int  1 2 3 4 5 6 7 8 9 10 ...
##  $ text        : chr  "@ayyytylerb that is so true drink lots of coffee" "RT @bryzy_brib: Senior March tmw morning at 7:25 A.M. in the SENIOR lot. Get up early, make yo coffee/breakfast"| __truncated__ "If you believe in #gunsense tomorrow would be a very good day to have your coffee any place BUT @Starbucks Guns"| __truncated__ "My cute coffee mug. http://t.co/2udvMU6XIG" ...
##  $ favorited   : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
##  $ replyToSN   : chr  "ayyytylerb" NA NA NA ...
##  $ created     : chr  "8/9/2013 2:43" "8/9/2013 2:43" "8/9/2013 2:43" "8/9/2013 2:43" ...
##  $ truncated   : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
##  $ replyToSID  : num  3.66e+17 NA NA NA NA ...
##  $ id          : num  3.66e+17 3.66e+17 3.66e+17 3.66e+17 3.66e+17 ...
##  $ replyToUID  : int  1637123977 NA NA NA NA NA NA 1316942208 NA NA ...
##  $ statusSource: chr  "<a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>" "<a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>" "web" "<a href=\"http://twitter.com/download/android\" rel=\"nofollow\">Twitter for Android</a>" ...
##  $ screenName  : chr  "thejennagibson" "carolynicosia" "janeCkay" "AlexandriaOOTD" ...
##  $ retweetCount: int  0 1 0 0 2 0 0 0 1 2 ...
##  $ retweeted   : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
##  $ longitude   : logi  NA NA NA NA NA NA ...
##  $ latitude    : logi  NA NA NA NA NA NA ...
```

```r
names(coffee)
```

```
##  [1] "num"          "text"         "favorited"    "replyToSN"   
##  [5] "created"      "truncated"    "replyToSID"   "id"          
##  [9] "replyToUID"   "statusSource" "screenName"   "retweetCount"
## [13] "retweeted"    "longitude"    "latitude"
```

```r
# Move the text column
text_num <- grep("text", names(coffee))
coffee <- coffee[,c(text_num, (1:ncol(coffee)[-text_num]))]
names(coffee)
```

```
##  [1] "text"         "num"          "text.1"       "favorited"   
##  [5] "replyToSN"    "created"      "truncated"    "replyToSID"  
##  [9] "id"           "replyToUID"   "statusSource" "screenName"  
## [13] "retweetCount" "retweeted"    "longitude"    "latitude"
```

```r
# Move the id column
column_id_num <- grep("id", names(coffee))
coffee <- coffee[,c(column_id_num, (1:ncol(coffee))[-column_id_num])]
names(coffee) <- c("doc_id", names(coffee)[-1])
names(coffee)
```

```
##  [1] "doc_id"       "text"         "num"          "text.1"      
##  [5] "favorited"    "replyToSN"    "created"      "truncated"   
##  [9] "replyToSID"   "replyToUID"   "statusSource" "screenName"  
## [13] "retweetCount" "retweeted"    "longitude"    "latitude"
```

Now the data frame can be made a source and then a VCorpus:


```r
coffee_source <- tm::DataframeSource(coffee)
coffee_corpus <- tm::VCorpus(coffee_source)
print(coffee_corpus)
```

```
## <<VCorpus>>
## Metadata:  corpus specific: 0, document level (indexed): 14
## Content:  documents: 1000
```

Now we will reclean it by using the previous function (remember `*`  jumps to a previous documnent):


```r
clean_corp <- clean_corpus(coffee_corpus)
```

Now we can create the document term matrix.


```r
# Create the document-term matrix from the corpus
coffee_dtm <- tm::DocumentTermMatrix(clean_corp)
coffee_tdm <- tm::TermDocumentMatrix(clean_corp)

# Print out coffee_dtm data
coffee_dtm
```

```
## <<DocumentTermMatrix (documents: 1000, terms: 3075)>>
## Non-/sparse entries: 7384/3067616
## Sparsity           : 100%
## Maximal term length: 27
## Weighting          : term frequency (tf)
```

```r
# Convert coffee_dtm to a matrix
coffee_m <- as.matrix(coffee_dtm)

# Print the dimensions of coffee_m
dim(coffee_m)
```

```
## [1] 1000 3075
```

```r
# Review a portion of the matrix to get some Starbucks
 # so documents 25 to 35, columns matching star or bucks.
coffee_m[25:35, c("star", "starbucks")]
```

```
##              Terms
## Docs          star starbucks
##   3.65664e+17    0         0
##   3.65664e+17    0         1
##   3.65664e+17    0         1
##   3.65664e+17    0         0
##   3.65664e+17    0         0
##   3.65664e+17    0         0
##   3.65664e+17    0         0
##   3.65664e+17    0         0
##   3.65664e+17    0         0
##   3.65664e+17    0         1
##   3.65664e+17    0         0
```



## (2) Word Clouds and More Interesting Visuals

## Common Text Mining Visuals

### Frequent Terms with `TM` 

First create the coffee *Term Document Matrix*:



```r
# Create the document-term matrix from the corpus
coffee_dtm <- tm::DocumentTermMatrix(clean_corp)
coffee_tdm <- tm::TermDocumentMatrix(clean_corp)
```
Now convert the `tdm` to a matrix: 


```r
coffee_m <- as.matrix(coffee_tdm)
str(coffee_m)
```

```
##  num [1:3075, 1:1000] 0 0 0 0 0 0 0 0 0 0 ...
##  - attr(*, "dimnames")=List of 2
##   ..$ Terms: chr [1:3075] "0630" "1000" "1026" "1030" ...
##   ..$ Docs : chr [1:1000] "3.65665e+17" "3.65665e+17" "3.65665e+17" "3.65665e+17" ...
```

```r
## summary(coffee_m)
## head(coffee_m)
```

Now in order to get the overall frequency of terms, take the sum of each row, which will give the total number of a term and then sort that by frequency



```r
# Calculate the row sums of coffee_m
term_frequency <- rowSums(coffee_m, na.rm=FALSE, dims = 1)

# Sort term_frequency in decreasing order
term_frequency <- sort(x = term_frequency, decreasing = TRUE)
```

Now view the top 10 most common Words


```r
head(term_frequency, 10)
```

```
##     like      cup     shop     just      get  morning     want drinking 
##      111      103       69       66       62       57       49       47 
##      can    looks 
##       45       45
```

Now plot a barchart of the 10 most common words


```r
barplot(term_frequency[1:5], col = "tan", las = 2)
```

![plot of chunk unnamed-chunk-20](figure/unnamed-chunk-20-1.png)

This can also be done in ggplot2:

>note that by default GGPlot2 will arrange the names by alphabetical order, to prevent this it is necessary to make sure that the the names are an `ordered` `factor` by using `df$name <- factor(df$name, levels=df$name, ordered=TRUE`, it doesn't matter whether this is specified before or after the order is applied, when the tibble is printed in should say `<ord>` as opposed to `<chr>` or `<fct>`. 


```r
# Create a Tibble
term_frequency.Tib <- tibble::enframe(term_frequency)

# Order the Tibble
## Using  Base Functions
term_frequencyDF <- data.frame("names" = names(term_frequency), "value" = term_frequency)
head(term_frequencyDF)
```

```
##           names value
## like       like   111
## cup         cup   103
## shop       shop    69
## just       just    66
## get         get    62
## morning morning    57
```

```r
term_frequency.Tib$name <- factor(term_frequencyDF$name, levels = term_frequencyDF$name[order(term_frequencyDF)], ordered = TRUE)
print(term_frequency.Tib)
```

```
## # A tibble: 3,075 x 2
##    name     value
##    <ord>    <dbl>
##  1 like       111
##  2 cup        103
##  3 shop        69
##  4 just        66
##  5 get         62
##  6 morning     57
##  7 want        49
##  8 drinking    47
##  9 can         45
## 10 looks       45
## # … with 3,065 more rows
```

```r
## Using dplyr Arrange
term_frequency.Tib <- tibble::enframe(term_frequency)
term_frequency.Tib <- dplyr::arrange(term_frequency.Tib, desc(value))
term_frequency.Tib$name <- factor(term_frequency.Tib$name, ordered = TRUE, levels = term_frequency.Tib$name)
term_frequency.Tib
```

```
## # A tibble: 3,075 x 2
##    name     value
##    <ord>    <dbl>
##  1 like       111
##  2 cup        103
##  3 shop        69
##  4 just        66
##  5 get         62
##  6 morning     57
##  7 want        49
##  8 drinking    47
##  9 can         45
## 10 looks       45
## # … with 3,065 more rows
```

```r
### Using dplyr Group by
# Sort the first few terms of the Tibble
# sort(x = term_frequency.Tib[1:10,]$value, decreasing=TRUE)
#term_frequency_Desc <- term_frequency.Tib[1:10,] %>% group_by(value)

# Use GGplot2 To plot them

ggplot(data = term_frequency.Tib[1:10,], aes(x = name, y = value)) +
       geom_col(col = "IndianRed", fill = "lightBlue") +
       theme_bw()
```

![plot of chunk unnamed-chunk-21](figure/unnamed-chunk-21-1.png)

```r
   labs(x = "Word", y = "Frequency", title = "Frequency of Word Use")
```

```
## $x
## [1] "Word"
## 
## $y
## [1] "Frequency"
## 
## $title
## [1] "Frequency of Word Use"
## 
## attr(,"class")
## [1] "labels"
```

```r
# If you do have trouble creating ordered factors an ugly hack to force the order is:

#ggplot(data = term_frequency_Desc, aes(x = reorder(name, -value), y = value)) +
#       geom_col(col = "IndianRed", fill = "lightBlue") +
#       theme_bw()
#   labs(x = "Word", y = "Frequency", title = "Frequency of Word Use")
```





## (3) Adding to Text-Mining Skills

## (4) Battle of the Tech Giants 







