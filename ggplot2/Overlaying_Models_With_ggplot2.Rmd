---
title: "Overlaying Models on GGPlot2"
output: html_notebook
---

# Overlaying Models onto ggplot2

So bascially the idea is to have two DataFrames:

1. The original sample / observation
2. A data Frame with the Model Predictions for various values

This way the Models can Have a corresponding name for the model type and be used seperately in the `geom_line` layer (as opposed to being sucked into the `geom_point` layer)

## Preamble

```{r}
# Preamble
## Install Pacman
load.pac <- function() {
  
  if(require("pacman")){
    library(pacman)
  }else{
    install.packages("pacman")
    library(pacman)
  }
  
  pacman::p_load(xts, sp, gstat, ggplot2, rmarkdown, reshape2, ggmap,
                 parallel, dplyr, plotly, tidyverse, reticulate, UsingR, Rmpfr,
                 swirl, corrplot, gridExtra, mise, boot)
  
}

load.pac()
mise()
```


## Make some Data
So first some sample data to play with:

```{r}

set.seed(88)

# Make some data
x <- seq(from = 1, to = 8, length.out = 1000)
y <- 1.8*(x-2)^3 - 10.1* (x-2)^2 - 2.8*x + 9.2 + rnorm(length(x), mean = 0, sd = 15)
mydata <- tibble("input" = x, "output" = y)
```

## Create Some Models

### Using GLM and a List

If the `glm` functoin is used without passing a `family` argument, linear regression wil be performed, so the following will produce the exact same output:

 * `lm(output ~ input, mydata) %>% coef`
 * `glm(output ~ input, mydata) %>% coef`

The benefit of using the `glm` function is that it can be used with the `cv.glm` function to easily perform cross validation (from Leave-One-Out Cross-Validation up to an arbitrary number of folds).

```{r}
## Models
 ## Do not reuse the name of the argument, it will break cv.glm

mods <- list()
# for (i in 1:7) {
#   model_formula <- paste0("output ~ poly(input, degree =", i, ")")
#   model_formula <- format(model_formula) # This isn't necessary
#    mods[[i]] <- glm(formula = model_formula, data = mydata)
# }


for (i in 1:7) {
mods[[i]] <- glm(formula = output ~ poly(input, i = i, raw = TRUE), data = mydata)
}


## Predictions
 ### Be super careful of the index here, input variable should be i=1 and lm should be i=2 and x^2 as i=3
preds <- tibble("input" = seq(from = 0, to = max(mydata$input), length.out = 1000))
preds[,1]
for (i in 1:length(mods)) {
 preds[,(i+1)] <- predict(mods[[i]], newdata = preds[,1])
 names(preds)[(i+1)] <- paste("p", i)
}

preds %>% head()

```

What's really strange and **really really worth noting**, is that, if you set the models using the loop above, `cv.glm` will not work, they need to be set by hand for that to work. To test this run all the code with this block commented out:

```{r}
 mods[[1]] <- glm(formula = output ~ poly(input, degree = 1, raw = TRUE), data = mydata)
 mods[[2]] <- glm(formula = output ~ poly(input, degree = 2, raw = TRUE), data = mydata)
 mods[[3]] <- glm(formula = output ~ poly(input, degree = 3, raw = TRUE), data = mydata)
 mods[[4]] <- glm(formula = output ~ poly(input, degree = 4, raw = TRUE), data = mydata)
 mods[[5]] <- glm(formula = output ~ poly(input, degree = 5, raw = TRUE), data = mydata)
 mods[[6]] <- glm(formula = output ~ poly(input, degree = 6, raw = TRUE), data = mydata)
 mods[[7]] <- glm(formula = output ~ poly(input, degree = 7, raw = TRUE), data = mydata)

```


### Using Variables and the `lm` function

If for some reason it was necessary to:

* Use a individual variables rather than a list

it would be necessary to `parse` the text and then `evaluate` the text, this is probably not the desirable way to do this, but for the sake of completion

```{r}
# Create Various Models
model_lm <- lm(output ~ input, data = mydata)
for (i in 2:9) {
 eval(
   parse(
     text = paste0("model_p", i, " <-  lm(output ~ poly(input, ", i, "), data = mydata)") 
     )
   )
}
```

## Predict some Corresponding Values

```{r}

# Create the Predictions

model_preds    <- tibble("input" = seq(from = 0, to = max(mydata$input)*0.9, length.out = 1000))
model_preds$lm <- predict(model_lm, newdata = model_preds[,1])
for (i in 2:7) {
 eval(
   parse(
     text = paste0("model_preds$p", i, " <-  predict(model_p", i, ", newdata = model_preds[,1])") 
     )
   )
}
model_preds %>% head()

```

## Make a tidy Data Set of Prediction Values

```{r}
# Make the Predictions tidy

# head(model_preds)
preds_tidy <- pivot_longer(data = preds, cols = names(preds[,-1]))
head(preds_tidy)

```

## Create the Plot by Overlaying the Model

### Create a Vector of Model Names

```{r}

     model_labels <- c("Linear", "Quadratic", "Cubic", "4th Degree",
                  "5th Degree", "6th Degree", "7th Degree") 
```


### Create the Plot

```{r}

# Overlay the Model

ggplot(data = mydata, aes(x = input, y = output)) +
  geom_point(size = 2, alpha = 0.9, col = "lightblue") +
  theme_classic() +
  labs(y = "Output Data / Observation", x = "Input Data", title = "Vehicle Breaking") +
  geom_line(data = preds_tidy, aes(x = input, y = value, col = name), lty =2, lwd = 1) +
  scale_color_discrete(name = "Model Type", labels = model_labels)
```

### Plot the RMSE

#### Training Error

For want of Completion plot the Model RMSE Values

```{r}
rmse <- function(x, model) {
  preds <- predict(model)
  sqrt(mean((x-preds)^2))
}

model_error <- data.frame("Model" = model_labels)

for (i in 1:length(mods)) {
  model_error$training[i] <- rmse(mydata$output, mods[[i]]) 
}

```

#### Expected Testing Error

The problem with using training error is that in theory it will always  improve in performance for a more flexible model, in order to overcome this cross validation is used as an estimate of the expected test data error.

The `cv.glm` function performs cross validation:

* the `K` argument is the number of folds
* the output `$delta` vector is a vector with the raw _**M**ean **S**quare **E**rror_ as the observation and the second being a value adjusted for bias.
  + The returned values are square error so use `sqrt()` to convert them to `rmse`.

```{r}
cv.glm(data = mydata, glmfit = glm(output ~ input, data = mydata), K = 10)$delta[1] %>% sqrt()
cv.glm(data = mydata, glmfit = glm(output ~ poly(input, degree = 1, raw = TRUE), data = mydata), K = 10)$delta[1] %>% sqrt()
cv.glm(data = mydata, glmfit = glm(output ~ poly(input, degree = 1, raw = FALSE), data = mydata), K = 10)$delta[1] %>% sqrt()
# mods[[1]] <- glm(output ~ poly(input, degree = 1, raw = FALSE), data = mydata)
cv.glm(data = mydata, glmfit = mods[[1]], K = 10)$delta[1] %>% sqrt()


library(boot)

## The column name specifies the input and response variable as inferred from the glm model
for (i in 1:length(mods)) {
model_error$testing[i] <- cv.glm(data = mydata, glmfit = mods[[i]], K = 10)$delta[1] %>% sqrt()
}


model_error

```


#### Visualise the Errors

```{r}

## make the Models an ordered factor
  ## For this to work the levels MUST be specified
model_error$Model <- factor(model_error$Model, levels = model_error$Model, ordered = TRUE)

## Make the data tidy
 ## If you want to print an assignment bracket it
(model_error_tidy <- pivot_longer(data = model_error, cols = c(training, testing)))

## Use GGPlot
## It is necessary to specify that the names sdelineate seperate groups
ggplot(data = model_error_tidy, aes(x = Model, y = value, fill = name, col = name, group = name)) +
#  geom_line() + 
  geom_col(position = "dodge") +
  theme_bw() +
  labs(x = "Model Type", y = "Root Mean Square Error", title = "Training and Testing Error") +
  guides(col = guide_legend("Error Type")) +
  scale_color_discrete(labels = c("Testing", "Training"))


```


I don't like these visualised errors, but, the method seems to be correct and maybe the distinctive u-shape would appear more pronounced with more variables or greater flexibility. 



